# Emotional-Cognition-Through-Voice-Tone-Analysis

Enhancing human-robot interactions through emotional cognition is pivotal for creating intuitive and responsive robotic systems. 
This study investigates the feasibility of utilizing voice tone as a reliable indicator of emotional states, specifically
distinguishing between angry and neutral emotions. 

Building on existing research in affective computing and benchmark machine learning models for real-time emotion detection, we 
employ key acoustic features such as Spectral Centroid Mean, Spectral Bandwidth Mean, RMS Mean, and Zero-Crossing Rate Mean to 
analyze vocal commands. Our experimental design involved analyzing two kinds of audio samples, angry and neutral toned, which 
were then normalized and examined through PCA and evaluated on effective meachine learning models. 

The results demonstrate significant differences in acoustic metrics, with the angry sample exhibiting higher spectral centroid and bandwidth 
values, as well as increased loudness and noisiness compared to the neutral sample. These findings indicate that distinct vocal features can 
effectively signal emotional states, enabling robots to adapt their responses accordingly. Such capabilities are crucial for applications in 
healthcare, customer service, and personal assistance, where empathetic and context-aware interactions enhance the overall user experience. 

This research contributes to the development of emotionally intelligent robotic systems, bridging the gap between functional support and meaningful
engagement in human-robot collaborations.

